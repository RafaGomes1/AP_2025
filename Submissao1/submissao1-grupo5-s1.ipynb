{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokeniza o texto convertendo-o para minúsculas e dividindo por espaços.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "def vectorize(text, vocab):\n",
    "    \"\"\"Converte um texto em um vetor de bag-of-words usando o vocabulário fornecido.\"\"\"\n",
    "    vector = np.zeros(len(vocab), dtype=np.float32)\n",
    "    for token in tokenize(text):\n",
    "        if token in vocab:\n",
    "            vector[vocab[token]] += 1\n",
    "    return vector\n",
    "\n",
    "def predict_logistic(X, weights, bias, threshold=0.5):\n",
    "    \"\"\"Faz predições com o modelo de regressão logística.\"\"\"\n",
    "    z = np.dot(X, weights) + bias\n",
    "    prob = 1 / (1 + np.exp(-z))\n",
    "    return (prob >= threshold).astype(int)\n",
    "\n",
    "def main():\n",
    "    # Caminho do modelo treinado\n",
    "    model_path = \"data/trained_logistic_chunked.npz\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Modelo não encontrado em: {model_path}\")\n",
    "\n",
    "    # Carregar os parâmetros do modelo e o vocabulário\n",
    "    model_data = np.load(model_path, allow_pickle=True)\n",
    "    weights = model_data[\"weights\"]\n",
    "    bias = model_data[\"bias\"]\n",
    "    vocab_tokens = model_data[\"vocab_tokens\"].tolist()\n",
    "    vocab_indices = model_data[\"vocab_indices\"].tolist()\n",
    "\n",
    "    # Reconstruir o vocabulário (token -> índice)\n",
    "    vocab = {token: idx for token, idx in zip(vocab_tokens, vocab_indices)}\n",
    "    print(f\"Vocabulário reconstituído: {len(vocab)} tokens.\")\n",
    "\n",
    "    # Caminho do CSV de input (com delimitador \";\")\n",
    "    input_csv = \"data/dataset2_inputs.csv\"\n",
    "    if not os.path.exists(input_csv):\n",
    "        raise FileNotFoundError(f\"Arquivo de input não encontrado: {input_csv}\")\n",
    "\n",
    "    # Ler o CSV forçando delimitador=\";\", ignorando linhas ruins\n",
    "    # Se o arquivo tiver alguma linha com colunas a mais, elas serão puladas\n",
    "    df_input = pd.read_csv(\n",
    "        input_csv,\n",
    "        delimiter=\";\",\n",
    "        engine=\"python\",\n",
    "        on_bad_lines=\"skip\"\n",
    "    )\n",
    "\n",
    "    # Verificar se as colunas são \"ID\" e \"Text\"\n",
    "    if \"ID\" not in df_input.columns or \"Text\" not in df_input.columns:\n",
    "        raise KeyError(\"O arquivo dataset2_inputs.csv deve conter as colunas 'ID' e 'Text'.\")\n",
    "\n",
    "    ids = df_input[\"ID\"].tolist()\n",
    "    texts = df_input[\"Text\"].astype(str).tolist()\n",
    "    print(f\"Dataset de input carregado com {len(texts)} textos (linhas válidas).\")\n",
    "    print(\"Colunas lidas:\", df_input.columns.tolist())\n",
    "\n",
    "    # Converter os textos para vetores utilizando o vocabulário\n",
    "    X_input = np.array([vectorize(text, vocab) for text in texts], dtype=np.float32)\n",
    "    print(\"Forma de X_input:\", X_input.shape)\n",
    "\n",
    "    # Fazer as predições usando o modelo treinado\n",
    "    preds = predict_logistic(X_input, weights, bias)\n",
    "    # Mapear 0 -> \"Human\", 1 -> \"AI\"\n",
    "    predicted_labels = [\"AI\" if p == 1 else \"Human\" for p in preds.flatten()]\n",
    "\n",
    "    # Criar o DataFrame de saída e salvar as predições\n",
    "    df_output = pd.DataFrame({\n",
    "        \"ID\": ids,\n",
    "        \"Predicted\": predicted_labels\n",
    "    })\n",
    "    output_csv = \"data/dataset2_predictions.csv\"\n",
    "    df_output.to_csv(output_csv, index=False, sep=\"\\t\")\n",
    "    print(f\"\\nArquivo de predições gerado: {output_csv}\")\n",
    "    print(df_output.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
