{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Converte o texto para minúsculas e separa em tokens.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab_from_saved(vocab_tokens, vocab_indices):\n",
    "    \"\"\"Reconstrói o vocabulário (token -> índice) a partir das listas salvas.\"\"\"\n",
    "    return {token: idx for token, idx in zip(vocab_tokens, vocab_indices)}\n",
    "\n",
    "def vectorize(text, vocab):\n",
    "    \"\"\"Converte um texto em um vetor bag-of-words usando o vocabulário fornecido.\"\"\"\n",
    "    vec = np.zeros(len(vocab), dtype=np.float32)\n",
    "    for token in tokenize(text):\n",
    "        if token in vocab:\n",
    "            vec[vocab[token] - 1] += 1  # -1, pois salvamos índices começando em 1\n",
    "    return vec\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def predict_mlp(X, model_params):\n",
    "    \"\"\"Realiza a predição usando o modelo MLP treinado.\"\"\"\n",
    "    W1 = model_params[\"W1\"]\n",
    "    b1 = model_params[\"b1\"]\n",
    "    W2 = model_params[\"W2\"]\n",
    "    b2 = model_params[\"b2\"]\n",
    "\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = np.maximum(0, Z1)  # ReLU\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    preds = (A2 >= 0.5).astype(int)\n",
    "    return preds\n",
    "\n",
    "def main():\n",
    "    # Carregar o modelo salvo\n",
    "    model_path = \"data/trained_mlp_model.npz\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Modelo não encontrado: {model_path}\")\n",
    "\n",
    "    model_data = np.load(model_path, allow_pickle=True)\n",
    "    W1 = model_data[\"W1\"]\n",
    "    b1 = model_data[\"b1\"]\n",
    "    W2 = model_data[\"W2\"]\n",
    "    b2 = model_data[\"b2\"]\n",
    "    vocab_tokens = model_data[\"vocab_tokens\"].tolist()\n",
    "    vocab_indices = model_data[\"vocab_indices\"].tolist()\n",
    "\n",
    "    model_params = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "    vocab = build_vocab_from_saved(vocab_tokens, vocab_indices)\n",
    "    print(f\"Vocabulário carregado com {len(vocab)} tokens.\")\n",
    "\n",
    "    # Carregar o arquivo de inputs (dataset2_inputs.csv) com delimitador \";\"\n",
    "    input_file = \"data/dataset2_inputs.csv\"\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Arquivo de input não encontrado: {input_file}\")\n",
    "\n",
    "    df_input = pd.read_csv(input_file, delimiter=\";\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "    # Filtrar linhas cujo ID comece com \"D2-\"\n",
    "    df_input = df_input[df_input[\"ID\"].str.startswith(\"D2-\")]\n",
    "    ids = df_input[\"ID\"].tolist()\n",
    "    texts = df_input[\"Text\"].astype(str).tolist()\n",
    "    print(f\"Dataset de input carregado com {len(texts)} textos válidos.\")\n",
    "\n",
    "    # Vetorizar os textos usando o vocabulário\n",
    "    X_input = np.array([vectorize(txt, vocab) for txt in texts], dtype=np.float32)\n",
    "    print(\"Shape de X_input:\", X_input.shape)\n",
    "\n",
    "    # Fazer predições\n",
    "    preds = predict_mlp(X_input, model_params)\n",
    "    predicted_labels = [\"AI\" if p == 1 else \"Human\" for p in preds.flatten()]\n",
    "\n",
    "    # Salvar resultados\n",
    "    df_output = pd.DataFrame({\n",
    "        \"ID\": ids,\n",
    "        \"Predicted\": predicted_labels\n",
    "    })\n",
    "    output_file = \"data/dataset2_predictions.csv\"\n",
    "    df_output.to_csv(output_file, index=False, sep=\";\")\n",
    "    print(f\"Arquivo de predições gerado: {output_file}\")\n",
    "    print(df_output.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
