{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# =====================\n",
    "# 1. Sementes e setup\n",
    "# =====================\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# =====================\n",
    "# 2. Carregar dados\n",
    "# =====================\n",
    "def load_dataset(file_path, sep='\\t'):\n",
    "    return pd.read_csv(file_path, sep=sep, encoding='utf-8')\n",
    "\n",
    "X_train = load_dataset(\"../data/test_input.csv\")\n",
    "y_train = load_dataset(\"../data/test_output.csv\")\n",
    "\n",
    "X_val = load_dataset(\"../data/human_ai_input.csv\")\n",
    "y_val = load_dataset(\"../data/human_ai_output.csv\")\n",
    "\n",
    "X_test = load_dataset(\"../data/dataset3_inputs.csv\")\n",
    "ids = X_test[\"ID\"]\n",
    "\n",
    "# =====================\n",
    "# 3. Tokenização\n",
    "# =====================\n",
    "max_words = 15000\n",
    "max_len = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train[\"Text\"])\n",
    "\n",
    "def tokenize_pad(texts):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=max_len)\n",
    "\n",
    "X_train_pad = tokenize_pad(X_train[\"Text\"])\n",
    "X_val_pad = tokenize_pad(X_val[\"Text\"])\n",
    "X_test_pad = tokenize_pad(X_test[\"Text\"])\n",
    "\n",
    "# =====================\n",
    "# 4. Labels\n",
    "# =====================\n",
    "y_train = y_train[\"Label\"].map({\"AI\": 1, \"Human\": 0}).values\n",
    "y_val = y_val[\"Label\"].map({\"AI\": 1, \"Human\": 0}).values\n",
    "\n",
    "# =====================\n",
    "# 5. Modelo CNN para texto\n",
    "# =====================\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(max_len,)),\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# =====================\n",
    "# 6. Treinar\n",
    "# =====================\n",
    "history = model.fit(X_train_pad, y_train,\n",
    "                    epochs=8,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val_pad, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "# =====================\n",
    "# 7. Prever no dataset3\n",
    "# =====================\n",
    "preds = model.predict(X_test_pad)\n",
    "pred_labels = [\"AI\" if p > 0.5 else \"Human\" for p in preds.flatten()]\n",
    "\n",
    "# =====================\n",
    "# 8. Guardar submissão\n",
    "# =====================\n",
    "output_df = pd.DataFrame({\n",
    "    \"ID\": ids,\n",
    "    \"Label\": pred_labels\n",
    "})\n",
    "output_df.to_csv(\"../data/previsao-cnn-s2.csv\", sep=\"\\t\", index=False)\n",
    "print(\"✅ Ficheiro de submissão gerado: submissao2-grupo007-cnn.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
