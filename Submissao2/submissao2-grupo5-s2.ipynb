{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treinar modelo MLP para deteção de textos AI vs Human\n",
        "\n",
        "Este notebook treina um modelo MLP simples com embeddings para classificar textos como sendo gerados por IA ou por humanos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Embedding, Input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import initializers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Garantir reprodutibilidade\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para carregar datasets\n",
        "def load_dataset(path, sep='\\t'):\n",
        "    return pd.read_csv(path, sep=sep, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar datasets\n",
        "X_train = load_dataset(\"data/test_input.csv\")\n",
        "y_train = load_dataset(\"data/test_output.csv\")\n",
        "\n",
        "X_val = load_dataset(\"data/human_ai_input.csv\")\n",
        "y_val = load_dataset(\"data/human_ai_output.csv\")\n",
        "\n",
        "X_test = load_dataset(\"data/dataset3_inputs.csv\")\n",
        "ids = X_test[\"ID\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizar e padronizar textos\n",
        "max_words = 20000\n",
        "max_len = 500\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train[\"Text\"])\n",
        "\n",
        "def tokenize_pad(texts):\n",
        "    seq = tokenizer.texts_to_sequences(texts)\n",
        "    return pad_sequences(seq, maxlen=max_len)\n",
        "\n",
        "X_train_pad = tokenize_pad(X_train[\"Text\"])\n",
        "X_val_pad = tokenize_pad(X_val[\"Text\"])\n",
        "X_test_pad = tokenize_pad(X_test[\"Text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converter labels\n",
        "y_train = y_train[\"Label\"].map({\"AI\": 1, \"Human\": 0}).values\n",
        "y_val = y_val[\"Label\"].map({\"AI\": 1, \"Human\": 0}).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar modelo\n",
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Input((max_len,)),\n",
        "    Embedding(max_words, embedding_dim, embeddings_initializer=initializers.GlorotUniform(seed=44)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Treinar modelo\n",
        "history = model.fit(X_train_pad, y_train, epochs=15, batch_size=32,\n",
        "                    validation_data=(X_val_pad, y_val), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prever no dataset final\n",
        "preds = model.predict(X_test_pad)\n",
        "pred_labels = [\"AI\" if p > 0.5 else \"Human\" for p in preds.flatten()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar previsões\n",
        "output_df = pd.DataFrame({\n",
        "    \"ID\": ids,\n",
        "    \"Label\": pred_labels\n",
        "})\n",
        "output_df.to_csv(\"data/submissao2-grupo5-s2.csv\", sep='\\t', index=False)\n",
        "print(\"✅ Ficheiro de submissão gerado: submissao2-grupo5-s2.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
