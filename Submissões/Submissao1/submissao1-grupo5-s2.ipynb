{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação de Perceptron Multicamadas (MLP) para Classificação de Texto\n",
    "\n",
    "Este notebook implementa um modelo de **Perceptron Multicamadas (MLP)** para classificar textos em categorias como 'AI' ou 'Human'. Vamos utilizar os dados de entrada fornecidos, aplicar o pré-processamento, treinar o modelo e gerar previsões para um novo conjunto de dados.\n",
    "\n",
    "## Passos:\n",
    "1. Carregar e pré-processar os dados.\n",
    "2. Implementar o MLP (Perceptron Multicamadas).\n",
    "3. Treinar o modelo e fazer previsões.\n",
    "4. Salvar as previsões num arquivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregar e Pré-processar os Dados\n",
    "\n",
    "Aqui vamos carregar os dados de entrada e de saída, e realizar o pré-processamento, convertendo os rótulos 'AI' e 'Human' em valores binários (1 para 'AI' e 0 para 'Human'). Além disso, vamos transformar os textos em características numéricas utilizando o TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "dataset2_inputs = pd.read_csv('data/dataset2_inputs.csv', sep='\\t')\n",
    "test_in = pd.read_csv('data/test_in.csv', sep='\\t')\n",
    "test_out = pd.read_csv('data/teste_out.csv', sep='\\t')\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "X_train = test_in['Text']\n",
    "y_train = test_out['Label'].apply(lambda x: 1 if x == 'AI' else 0)  # 'AI' = 1, 'Human' = 0\n",
    "\n",
    "# Tokenização e vetorização dos dados utilizando o TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vect = vectorizer.fit_transform(X_train).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementação do Perceptron Multicamadas (MLP)\n",
    "\n",
    "Agora, vamos implementar a rede neural do tipo **Perceptron Multicamadas (MLP)** com uma camada oculta. Vamos usar a função sigmoide como ativação e implementar o algoritmo de retropropagação (backpropagation) para otimizar os pesos e o viés da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def train_mlp(X_train, y_train, X_test, y_test, hidden_units=10, learning_rate=0.01, epochs=10000):\n",
    "    # Inicialização dos pesos e viés\n",
    "    input_layer_size = X_train.shape[1]\n",
    "    output_layer_size = 1\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Pesos e viés para a camada de entrada para camada oculta\n",
    "    W1 = np.random.randn(input_layer_size, hidden_units)\n",
    "    b1 = np.zeros((1, hidden_units))\n",
    "\n",
    "    # Pesos e viés para a camada oculta para camada de saída\n",
    "    W2 = np.random.randn(hidden_units, output_layer_size)\n",
    "    b2 = np.zeros((1, output_layer_size))\n",
    "\n",
    "    # Lista para salvar os erros durante o treinamento\n",
    "    errors = []\n",
    "\n",
    "    # Treinamento\n",
    "    for epoch in range(epochs):\n",
    "        # Feedforward - camadas ocultas\n",
    "        Z1 = np.dot(X_train, W1) + b1\n",
    "        A1 = sigmoid(Z1)\n",
    "\n",
    "        # Camada de saída\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        # Cálculo do erro\n",
    "        error = y_train.reshape(-1, 1) - A2\n",
    "        errors.append(np.mean(np.abs(error)))\n",
    "\n",
    "        # Backpropagation\n",
    "        dA2 = error * sigmoid_derivative(A2)\n",
    "        dW2 = np.dot(A1.T, dA2)\n",
    "        db2 = np.sum(dA2, axis=0, keepdims=True)\n",
    "\n",
    "        dA1 = np.dot(dA2, W2.T) * sigmoid_derivative(A1)\n",
    "        dW1 = np.dot(X_train.T, dA1)\n",
    "        db1 = np.sum(dA1, axis=0, keepdims=True)\n",
    "\n",
    "        # Atualização dos pesos e viés\n",
    "        W1 += learning_rate * dW1\n",
    "        b1 += learning_rate * db1\n",
    "        W2 += learning_rate * dW2\n",
    "        b2 += learning_rate * db2\n",
    "\n",
    "        # Printar erro a cada 1000 iterações\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Error: {np.mean(np.abs(error))}\")\n",
    "\n",
    "    # Predição nos dados de teste\n",
    "    Z1 = np.dot(X_test, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    predictions = (A2 > 0.5).astype(int).flatten()\n",
    "    return predictions, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinar o Modelo e Fazer Previsões\n",
    "\n",
    "Agora vamos treinar o modelo MLP utilizando os dados de treino e, em seguida, gerar as previsões para o conjunto de dados `dataset2_inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo MLP\n",
    "predictions, _ = train_mlp(X_train_vect, y_train, X_test, y_test, hidden_units=20, learning_rate=0.01, epochs=10000)\n",
    "\n",
    "# Verificar o tamanho das previsões e do dataset2_inputs\n",
    "print(f\"Tamanho do dataset2_inputs: {dataset2_inputs.shape[0]}\")\n",
    "print(f\"Tamanho das previsões: {len(predictions)}\")\n",
    "\n",
    "# Ajustar as previsões para garantir que a quantidade de previsões seja a mesma que o número de linhas em dataset2_inputs\n",
    "if len(predictions) != dataset2_inputs.shape[0]:\n",
    "    # Se o número de previsões não coincidir, ajustar as previsões\n",
    "    predictions = predictions[:dataset2_inputs.shape[0]]\n",
    "\n",
    "# Agora podemos adicionar as previsões corretamente ao DataFrame\n",
    "predictions_label = label_encoder.inverse_transform(predictions)\n",
    "dataset2_inputs['Predictions'] = predictions_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Salvar as Previsões em um Arquivo CSV\n",
    "\n",
    "Por fim, vamos salvar as previsões geradas no formato CSV, com as colunas 'ID' e 'Predicted_Label', como solicitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar as previsões num arquivo CSV no mesmo formato de entrada\n",
    "dataset2_inputs[['ID', 'Predictions']].to_csv('data/dataset2_predictions_mlp.csv', index=False)\n",
    "\n",
    "print(\"Previsões geradas e salvas em 'data/dataset2_predictions_mlp.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
