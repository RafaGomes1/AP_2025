{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T15:43:12.332871Z",
     "start_time": "2025-03-24T15:42:38.942308Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  generated\n",
      "0  Cars. Cars have been around since they became ...        0.0\n",
      "1  Transportation is a large necessity in most co...        0.0\n",
      "2  \"America's love affair with it's vehicles seem...        0.0\n",
      "3  How often do you ride in a car? Do you drive a...        0.0\n",
      "4  Cars are a wonderful thing. They are perhaps o...        0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 2. Tokenização e vetorização dos dados utilizando CountVectorizer\u001b[39;00m\n\u001b[0;32m     18\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m X_vect \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 3. Dividir os dados em treino e teste (80% treino, 20% teste)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_vect, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:109\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc, stop_words)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:242\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# handle token n-grams\u001b[39;00m\n\u001b[0;32m    245\u001b[0m min_n, max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Carregar e Pré-processar o Dataset \"AI_human.csv\"\n",
    "df = pd.read_csv('data/AI_Human.csv')\n",
    "\n",
    "# Verificar as primeiras linhas para entender a estrutura dos dados\n",
    "print(df.head())\n",
    "\n",
    "# Separar os dados em X (texto) e y (rótulos)\n",
    "X = df['text']  # Coluna com os textos\n",
    "y = df['generated'].apply(lambda x: 1 if x == 1 else 0)  # 'AI' = 1, 'Human' = 0\n",
    "\n",
    "# 2. Tokenização e vetorização dos dados utilizando CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "X_vect = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# 3. Dividir os dados em treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Funções para a Regressão Logística\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression(X, y, learning_rate=0.01, epochs=100):\n",
    "    m, n = X.shape\n",
    "    W = np.zeros(n)\n",
    "    b = 0\n",
    "\n",
    "    # Descida do gradiente\n",
    "    for epoch in range(epochs):\n",
    "        model = np.dot(X, W) + b\n",
    "        predictions = sigmoid(model)\n",
    "\n",
    "        dw = (1/m) * np.dot(X.T, (predictions - y))\n",
    "        db = (1/m) * np.sum(predictions - y)\n",
    "\n",
    "        # Atualização dos parâmetros\n",
    "        W -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "\n",
    "    return W, b\n",
    "\n",
    "def predict(X, W, b):\n",
    "    model = np.dot(X, W) + b\n",
    "    predictions = sigmoid(model)\n",
    "    return [1 if p >= 0.5 else 0 for p in predictions]\n",
    "\n",
    "# 5. Treinar o modelo de Regressão Logística no dataset \"AI_human.csv\"\n",
    "W, b = logistic_regression(X_train, y_train, learning_rate=0.01, epochs=1000)\n",
    "\n",
    "# 6. Avaliar o modelo no conjunto de teste (usado para validação)\n",
    "predictions_test = predict(X_test, W, b)\n",
    "\n",
    "# 7. Carregar o dataset2_inputs.csv e pré-processar\n",
    "dataset2_inputs = pd.read_csv('data/dataset2_inputs.csv', sep='\\t')\n",
    "\n",
    "# Verificar a estrutura do dataset2_inputs e adaptar o pré-processamento\n",
    "X_input = dataset2_inputs['Text']\n",
    "X_input_vect = vectorizer.transform(X_input).toarray()  # Transformar o texto de dataset2_inputs\n",
    "\n",
    "# 8. Fazer previsões usando o modelo treinado no dataset2_inputs\n",
    "predictions = predict(X_input_vect, W, b)\n",
    "\n",
    "# 9. Mapear as previsões para os rótulos ('AI' ou 'Human')\n",
    "predicted_labels = ['AI' if p == 1 else 'Human' for p in predictions]\n",
    "\n",
    "# 10. Criar o DataFrame de saída com ID e as previsões\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': dataset2_inputs['ID'],  # Usando os IDs de dataset2_inputs\n",
    "    'Predicted_Label': predicted_labels\n",
    "})\n",
    "\n",
    "# 11. Salvar as previsões em um arquivo CSV\n",
    "output_df.to_csv('data/AI_HUMAN_dataset2_predictions_logistic-s1.csv', index=False)\n",
    "\n",
    "print(\"Previsões geradas e salvas em 'data/AI_HUMAN_dataset2_predictions_logistic.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
