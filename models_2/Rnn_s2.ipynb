{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "94/94 [==============================] - 33s 337ms/step - loss: 0.2707 - accuracy: 0.9120 - val_loss: 0.0152 - val_accuracy: 0.9990\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 14s 148ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9990\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 15s 155ms/step - loss: 7.1642e-04 - accuracy: 1.0000 - val_loss: 6.2416e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 13s 140ms/step - loss: 2.6701e-04 - accuracy: 1.0000 - val_loss: 4.9760e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 15s 161ms/step - loss: 1.4847e-04 - accuracy: 1.0000 - val_loss: 8.6069e-04 - val_accuracy: 1.0000\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "✅ Ficheiro de submissão gerado: submissao2-grupo007-rnn.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "# =====================\n",
    "# 1. Sementes e setup\n",
    "# =====================\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# =====================\n",
    "# 2. Carregar dados\n",
    "# =====================\n",
    "def load_dataset(file_path, sep='\\t'):\n",
    "    return pd.read_csv(file_path, sep=sep, encoding='utf-8')\n",
    "\n",
    "X_train = load_dataset(\"../data/test_input.csv\")\n",
    "y_train = load_dataset(\"../data/test_output.csv\")\n",
    "\n",
    "X_val = load_dataset(\"../data/human_ai_input.csv\")\n",
    "y_val = load_dataset(\"../data/human_ai_output.csv\")\n",
    "\n",
    "X_test = load_dataset(\"../data/dataset3_inputs.csv\")\n",
    "ids = X_test[\"ID\"]\n",
    "\n",
    "# =====================\n",
    "# 3. Tokenização\n",
    "# =====================\n",
    "max_words = 15000\n",
    "max_len = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train[\"Text\"])\n",
    "\n",
    "def tokenize_pad(texts):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return preprocessing.sequence.pad_sequences(seqs, maxlen=max_len)\n",
    "\n",
    "X_train_pad = tokenize_pad(X_train[\"Text\"])\n",
    "X_val_pad = tokenize_pad(X_val[\"Text\"])\n",
    "X_test_pad = tokenize_pad(X_test[\"Text\"])\n",
    "\n",
    "# =====================\n",
    "# 4. Preparar labels\n",
    "# =====================\n",
    "y_train = y_train[\"Label\"].map({\"AI\": 1, \"Human\": 0}).values\n",
    "y_val = y_val[\"Label\"].map({\"AI\": 1, \"Human\": 0}).values\n",
    "\n",
    "# =====================\n",
    "# 5. Criar modelo RNN (LSTM)\n",
    "# =====================\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(max_len,)),\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim,\n",
    "              embeddings_initializer=initializers.GlorotUniform(seed=44)),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# =====================\n",
    "# 6. Treinar\n",
    "# =====================\n",
    "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=32,\n",
    "                    validation_data=(X_val_pad, y_val), verbose=1)\n",
    "\n",
    "# =====================\n",
    "# 7. Prever para dataset3\n",
    "# =====================\n",
    "preds = model.predict(X_test_pad)\n",
    "pred_labels = [\"AI\" if p > 0.5 else \"Human\" for p in preds.flatten()]\n",
    "\n",
    "# =====================\n",
    "# 8. Exportar resultados\n",
    "# =====================\n",
    "output_df = pd.DataFrame({\n",
    "    \"ID\": ids,\n",
    "    \"Label\": pred_labels\n",
    "})\n",
    "\n",
    "output_df.to_csv(\"../data/previsao-rnn-s2.csv\", sep=\"\\t\", index=False)\n",
    "print(\"✅ Ficheiro de submissão gerado: submissao2-grupo007-rnn.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
